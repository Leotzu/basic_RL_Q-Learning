{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "q-learning_shortest_path.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN4zVXSVEWgxWTIaX1pq1Yy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Leotzu/basic_RL_Q-Learning/blob/master/q_learning_shortest_path.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "D3X5QFzbvOT0"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the agent in the environment\n",
        "def train(alpha, gamma, iterations, location_to_state, state_to_location, actions, reward_env, check_point):\n",
        "\n",
        "    # initialize Q table with all zeros\n",
        "    q_table = np.zeros_like(reward_env)\n",
        "\n",
        "    for i in range(1, iterations+1):\n",
        "        # pick a random current state \n",
        "        current_state = np.random.randint(0, 9)\n",
        "        playable_actions = []\n",
        "\n",
        "        '''\n",
        "        Iterate through the rewards matrix to get the states which\n",
        "        are directly reachable from the randomly chosen current state.\n",
        "        Then add those states to a list called playable_actions.\n",
        "        '''\n",
        "        for j in range(9):\n",
        "            if reward_env[current_state, j] > 0:\n",
        "                playable_actions.append(j)\n",
        "\n",
        "        # if current location has no out-going edges, skip\n",
        "        if len(playable_actions) == 0:\n",
        "            continue\n",
        "\n",
        "        # randomly choose the one of the reachable states as the next state\n",
        "        next_state = np.random.choice(playable_actions)\n",
        "\n",
        "        # Bellman's Equation (finding temporal difference)\n",
        "        q_table[current_state, next_state] += alpha * (reward_env[current_state, next_state] \\\n",
        "                                           + gamma * q_table[next_state, np.argmax(q_table[next_state,])] \\\n",
        "                                           - q_table[current_state, next_state])\n",
        "        \n",
        "        # Print out the Q-table at every check_point\n",
        "        if check_point > 0:\n",
        "            if i % check_point == 0:\n",
        "                print(f'Q-table at iteration {i}:\\n {q_table}\\n')\n",
        "        \n",
        "    return q_table\n",
        "\n",
        "# function to print the shortest path\n",
        "def get_optimal_route(start_location, end_location, q_table, location_to_state, state_to_location):\n",
        "    # initialize the route and next_location\n",
        "    route = [start_location]\n",
        "    next_location = start_location\n",
        "    current_location = start_location\n",
        "\n",
        "    # append the next location to the route list then print that list\n",
        "    while next_location != end_location:\n",
        "        current_state = location_to_state[current_location]\n",
        "\n",
        "        # to avoid infinite loops, remove reflexive reward at current_state\n",
        "        tmp_q_table = np.copy(q_table)\n",
        "        tmp_q_table[current_state, current_state] = 0\n",
        "\n",
        "        # if there are no exiting edges, path does not exist\n",
        "        if max(tmp_q_table[current_state,]) == 0:\n",
        "            print(f'Path does not exit from {start_location} to {end_location}')\n",
        "            return 0\n",
        "        next_state = np.argmax(tmp_q_table[current_state,])\n",
        "        next_location = state_to_location[next_state]\n",
        "        route.append(next_location)\n",
        "        current_location = next_location\n",
        "\n",
        "    print(route)"
      ],
      "metadata": {
        "id": "jButeYgaPXhH"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the states\n",
        "location_to_state = {\n",
        "    'L1': 0,\n",
        "    'L2': 1,\n",
        "    'L3': 2,\n",
        "    'L4': 3,\n",
        "    'L5': 4,\n",
        "    'L6': 5,\n",
        "    'L7': 6,\n",
        "    'L8': 7,\n",
        "    'L9': 8\n",
        "}\n",
        "\n",
        "# Define the environment\n",
        "env = np.array([[0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
        "                [1, 0, 0, 1, 0, 0, 0, 0, 0],\n",
        "                [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
        "                [0, 0, 0, 0, 1, 1, 0, 0, 0],\n",
        "                [0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
        "                [0, 0, 0, 1, 0, 0, 0, 0, 1],\n",
        "                [0, 0, 1, 0, 0, 0, 0, 0, 1],\n",
        "                [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "                [0, 0, 0, 0, 0, 1, 1, 1, 0]])\n",
        "\n",
        "# Define the actions\n",
        "actions = [i for i in range(len(env))]\n",
        "\n",
        "# Create a dictionary mapping from state to location\n",
        "state_to_location = dict((state, location) for location, state in location_to_state.items())"
      ],
      "metadata": {
        "id": "4mUnLuniKVMf"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set hyperparameters\n",
        "\n",
        "alpha = 0.9  # learning rate\n",
        "gamma = 0.75  # Discount factor\n",
        "iterations = 1000\n",
        "\n",
        "# define reward\n",
        "end_location = 'L9'\n",
        "end_reward = 1000\n",
        "\n",
        "reward_structure = np.zeros_like(env)\n",
        "end_state = location_to_state[end_location]\n",
        "reward_structure[end_state, end_state] = end_reward\n",
        "\n",
        "# additional reward\n",
        "'''\n",
        "prefered_location = 'L4'\n",
        "prefered_location_reward = 800\n",
        "prefered_state = location_to_state[prefered_location]\n",
        "reward_structure[prefered_state, prefered_state] = prefered_location_reward\n",
        "'''\n",
        "\n",
        "# combine the reward and env map\n",
        "reward_env = np.add(env, reward_structure)\n",
        "print(f'Environment with reward structure:\\n {reward_env}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ponQu57ZJ400",
        "outputId": "880acce8-08fe-460a-dbf9-e3bdce9b71ea"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment with reward structure:\n",
            " [[   0    1    1    0    0    0    0    0    0]\n",
            " [   1    0    0    1    0    0    0    0    0]\n",
            " [   1    0    0    0    0    0    1    0    0]\n",
            " [   0    0    0    0    1    1    0    0    0]\n",
            " [   0    1    0    0    0    0    0    0    0]\n",
            " [   0    0    0    1    0    0    0    0    1]\n",
            " [   0    0    1    0    0    0    0    0    1]\n",
            " [   0    0    0    0    0    0    0    0    0]\n",
            " [   0    0    0    0    0    1    1    1 1000]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "\n",
        "# Q table will be printed when training iteration = check_point\n",
        "check_point = 1000\n",
        "\n",
        "q_table = train(alpha, gamma, iterations, location_to_state, state_to_location, actions, reward_env, check_point)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fc2z6TEuRK8J",
        "outputId": "ef43966a-8f16-4912-e3b2-e06c6f8c0763"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q-table at iteration 1000:\n",
            " [[   0 1264 1686    0    0    0    0    0    0]\n",
            " [1265    0    0 1685    0    0    0    0    0]\n",
            " [1265    0    0    0    0    0 2247    0    0]\n",
            " [   0    0    0    0  948 2247    0    0    0]\n",
            " [   0 1264    0    0    0    0    0    0    0]\n",
            " [   0    0    0 1685    0    0    0    0 2996]\n",
            " [   0    0 1686    0    0    0    0    0 2996]\n",
            " [   0    0    0    0    0    0    0    0    0]\n",
            " [   0    0    0    0    0 2247 2244    0 3994]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print shortest path between any two locations\n",
        "start_location = 'L1'\n",
        "\n",
        "get_optimal_route(start_location, end_location, q_table, location_to_state, state_to_location)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0Tad_pyRc89",
        "outputId": "3faeffd9-8be7-4cda-d13e-599e3c1c6763"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['L1', 'L3', 'L7', 'L9']\n"
          ]
        }
      ]
    }
  ]
}